{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_datareader in c:\\users\\saeed\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas_datareader) (1.1.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas_datareader) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas_datareader) (4.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas>=0.23->pandas_datareader) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas>=0.23->pandas_datareader) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from pandas>=0.23->pandas_datareader) (2.8.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pandas_datareader) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pandas_datareader) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pandas_datareader) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pandas_datareader) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas_datareader) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: six in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TensorFlow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (1.19.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (2.10.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (0.12.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (3.15.8)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (3.7.4.3)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (0.35.1)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from TensorFlow) (1.15.0)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->TensorFlow) (50.3.1.post20201107)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->TensorFlow) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.0-py3-none-any.whl (2.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->TensorFlow) (2.24.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.0-py2.py3-none-any.whl (146 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->TensorFlow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->TensorFlow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->TensorFlow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->TensorFlow) (3.0.4)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33700 sha256=037c94087435194c4ebafe4a6276a925fe316a6bc2bcd06c026e0b3201cd4084\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=fd877c91f969b4f5b12dac1a4edae1fa4cd5e05cefc1e270a8036d140ed03e6a\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: astunparse, gast, google-pasta, tensorflow-estimator, wrapt, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, markdown, tensorboard-data-server, tensorboard-plugin-wit, tensorboard, keras-preprocessing, termcolor, opt-einsum, flatbuffers, TensorFlow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed TensorFlow-2.4.1 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.3.3 google-auth-1.30.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.0 tensorboard-plugin-wit-1.8.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anvil-uplink\n",
      "  Downloading anvil_uplink-0.3.36-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: future in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from anvil-uplink) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\saeed\\anaconda3\\lib\\site-packages (from anvil-uplink) (1.15.0)\n",
      "Collecting ws4py\n",
      "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: ws4py\n",
      "  Building wheel for ws4py (setup.py): started\n",
      "  Building wheel for ws4py (setup.py): finished with status 'done'\n",
      "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45221 sha256=957d280a4b29601cc19bd484eb817308e603d37e8ae6c68566d2f447b9d87881\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\ea\\f9\\a1\\34e2943cce3cf7daca304bfc35e91280694ced9194a487ce2f\n",
      "Successfully built ws4py\n",
      "Installing collected packages: ws4py, argparse, anvil-uplink\n",
      "Successfully installed anvil-uplink-0.3.36 argparse-1.4.0 ws4py-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c27949dfb4be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas_datareader as web\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend.tensorflow_backend as tb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as tb\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.request\n",
    "import re\n",
    "import anvil.mpl_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b5de9ec5f8e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import anvil.server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# anvil.server.connect(\"L2BCBDUVFH3TVGLN5T6GUD4A-YRRXCIBEM56NA64X\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_source\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'yahoo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2018-01-1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoday_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_value' is not defined"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"L2BCBDUVFH3TVGLN5T6GUD4A-YRRXCIBEM56NA64X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "@anvil.server.callable\n",
    "def predict(input_value):\n",
    "    tb._SYMBOLIC_SCOPE.value = True\n",
    "    price = PredictPrice(input_value)\n",
    "    date = datetime.date(datetime.now())\n",
    "    tomorrow = date + timedelta(days = 1)\n",
    "    #news[] = articles(input_value, 3)\n",
    "    #visualize data\n",
    "    return tomorrow, price\n",
    "\n",
    "@anvil.server.callable\n",
    "def graph_data(input_value):\n",
    "    today_date = datetime.date(datetime.now())\n",
    "    df = web.DataReader(input_value, data_source='yahoo',start='2018-01-1', end=today_date)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('Price History')\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel('Close price USD ($)', fontsize=18)\n",
    "    plt.plot(df['Close'])\n",
    "\n",
    "    return anvil.mpl_util.plot_image()\n",
    "\n",
    "def PredictPrice(input_value):\n",
    "    today_date = datetime.date(datetime.now())\n",
    "    df = web.DataReader(input_value, data_source='yahoo',start='2018-01-1', end=today_date)\n",
    "    #create a new df wtih close column\n",
    "    data = df.filter(['Close'])\n",
    "    #convert df to numpy array\n",
    "    dataset = data.values\n",
    "    #get number of rows to train the model on\n",
    "    training_data_len = math.ceil(len(dataset)*.8)\n",
    "    #scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(dataset) #computes min and max values to be used for scaling and transform data based on those values range [0,1]\n",
    "    #create the training data set\n",
    "    #create the scaled training dataset\n",
    "    train_data = scaled_data[0:training_data_len, :]\n",
    "    #split data into x_train and y_train data sets\n",
    "    x_train = []\n",
    "    y_train = [] #target var\n",
    "\n",
    "    for i in range(60, len(train_data)):\n",
    "        x_train.append(train_data[i-60:i, 0])\n",
    "        y_train.append(train_data[i, 0])\n",
    "    #convert the x_train and y_train to numpy arr\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    #reshape data LSTM model requires 3D\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    #build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    #compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    #train the model\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "    #create the testing dataset\n",
    "    # get the quote\n",
    "    new_df = df.filter(['Close'])\n",
    "    #get last 60 day closing price values .values converts to array\n",
    "    last_60_days = new_df[-60:].values\n",
    "    last_60_days_scaled = scaler.transform(last_60_days)\n",
    "    #create empty list\n",
    "    X_test = []\n",
    "    #append past 60 days to it\n",
    "    X_test.append(last_60_days_scaled)\n",
    "    #convert the X_test to numpy array\n",
    "    X_test = np.array(X_test)\n",
    "    #reshape 2D -> 3D\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    #get predicted price\n",
    "    pred_price = model.predict(X_test)\n",
    "    #undo scaling\n",
    "    pred_price = scaler.inverse_transform(pred_price)\n",
    "    #predicted = predict_price(input_value)\n",
    "    p = str(pred_price)\n",
    "    price = p.strip('[]')\n",
    "#     #plot data\n",
    "#     train = data[:training_data_len]\n",
    "#     valid = data[training_data_len:]\n",
    "#     valid['Predictions']= predictions\n",
    "    return price\n",
    "@anvil.server.callable\n",
    "#searches for articles based on searchTerm variable\n",
    "def articles(searchTerm):\n",
    "    NumberofArticles =3\n",
    "    url=\"https://news.search.yahoo.com/search?p=\"+searchTerm\n",
    "    link = urllib.request.urlopen(url)\n",
    "    htmlbytes = link.read()\n",
    "    html = htmlbytes.decode(\"utf-8\")\n",
    "    link.close()\n",
    "    arr=[]\n",
    "    results=re.findall(\"\\\"https:\\/\\/[^\\\"]+\\\" referrerpolicy=\\\"origin\\\"\", html)\n",
    "    #removing irrelevent links\n",
    "    for string in results:\n",
    "        if not(string.endswith(\"png\\\"\")):\n",
    "            arr.append(string)\n",
    "    #removing duplicate links\n",
    "    arr=list(dict.fromkeys(arr))\n",
    "    News=[]\n",
    "    #condensing links into properlly formatted links to news articles\n",
    "    for string in arr:   \n",
    "        News.append(string[1:len(string)-25])\n",
    "    #condensing the array down to NumberofArticles: this wont reduce the array if the number is greater then the size News[]\n",
    "    releventNews=[]\n",
    "    i=0\n",
    "    while len(releventNews)<NumberofArticles:\n",
    "        releventNews.append(News[i])\n",
    "        i+=1\n",
    "    return releventNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fe7dfe0c5ebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# arts = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0marts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"INSERT INTO article_list VALUES (?,?)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnews\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fe7dfe0c5ebe>\u001b[0m in \u001b[0;36marticles\u001b[1;34m(searchTerm)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreleventNews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mNumberofArticles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mreleventNews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreleventNews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def articles(searchTerm):\n",
    "    NumberofArticles =3\n",
    "    url=\"https://news.search.yahoo.com/search?p=\"+searchTerm\n",
    "    link = urllib.request.urlopen(url)\n",
    "    htmlbytes = link.read()\n",
    "    html = htmlbytes.decode(\"utf-8\")\n",
    "    link.close()\n",
    "    arr=[]\n",
    "    results=re.findall(\"\\\"https:\\/\\/[^\\\"]+\\\" referrerpolicy=\\\"origin\\\"\", html)\n",
    "    #removing irrelevent links\n",
    "    for string in results:\n",
    "        if not(string.endswith(\"png\\\"\")):\n",
    "            arr.append(string)\n",
    "    #removing duplicate links\n",
    "    arr=list(dict.fromkeys(arr))\n",
    "    News=[]\n",
    "    #condensing links into properlly formatted links to news articles\n",
    "    for string in arr:   \n",
    "        print('test')\n",
    "        print(string)\n",
    "        News.append(string[1:len(string)-25])\n",
    "    #condensing the array down to NumberofArticles: this wont reduce the array if the number is greater then the size News[]\n",
    "    releventNews=[]\n",
    "    i=0\n",
    "    while len(releventNews)<NumberofArticles:\n",
    "        releventNews.append(News[i])\n",
    "        i+=1\n",
    "    return releventNews\n",
    "\n",
    "\n",
    "#create database\n",
    "connection = sqlite3.connect('stocks.db')\n",
    "#create cursor instance\n",
    "cursor = connection.cursor()\n",
    "\n",
    "company_list = ['apple', 'amazon', 'bestbuy', 'facebook', 'tesla', 'alphabet', 'boeing', 'twitter', 'palantir', 'walmart']\n",
    "\n",
    "arts = []\n",
    "for company in company_list:\n",
    "    # arts = []\n",
    "    print(company)\n",
    "    arts = articles(company)\n",
    "    query = \"INSERT INTO article_list VALUES (?,?)\"\n",
    "    for news in arts:\n",
    "        print(news)\n",
    "        print('--')\n",
    "    print('--------')\n",
    "    # cursor.executemany(query , data)\n",
    "    # index +=1\n",
    "    # data = []\n",
    "    # connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
